{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Line Search Methods (Week 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step Length\n",
    "\n",
    "Each iteration of line search methods requires a step length $\\alpha_k$ and a search direction $p_k$ to be computed, the update is\n",
    "\n",
    "$$x_{k+1} = x_k + \\alpha_k p_k.$$\n",
    "\n",
    "Once the search direction $p_k$ is computed, the step length $\\alpha_k$ is then computed to reduce the objective function $f$. Usually, $\\alpha_k$ should be compromised between the reduction of $f$ and the computational cost of computing $\\alpha_k$. Ideally, the best choice is the so-called ``exact line search'' which finds the optimal $\\alpha_k$ that minimizes the following single-variable function $\\phi(\\cdot)$ by\n",
    "\n",
    "$$\\phi(\\alpha) := f(x_k + \\alpha p_k),\\quad \\alpha > 0.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import norm\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "\"\"\"\n",
    "@description: exact line search steepest decent method \n",
    "@parameters : \n",
    "    @objFunc    : objective function  \n",
    "    @gradObjFunc: gradient of objective function\n",
    "    @x0         : starting point \n",
    "    @tol        : tolerance for stopping criteria \n",
    "    @maxIter    : maximum iteration for stopping criteria\n",
    "\"\"\"\n",
    "def exact_steepest_decent_method(objFunc, gradObjFunc, x0, tol, maxIter):\n",
    "    path      = [x0]\n",
    "    k         = 0\n",
    "    xk        = x0\n",
    "    pk        = -gradObjFunc(x0)\n",
    "    while norm(pk) > tol and k <= maxIter:\n",
    "        def subproblem1D(alpha):\n",
    "            return objFunc(xk + alpha * pk)\n",
    "        res = minimize_scalar(subproblem1D) \n",
    "        xk  = xk + res.x * pk \n",
    "        pk  = -gradObjFunc(xk)\n",
    "        k   = k + 1\n",
    "        path.append(xk)\n",
    "\n",
    "    path = np.array(path) # convert to array\n",
    "        \n",
    "    if norm(pk) <= tol:\n",
    "        print(\"Found the minimizer at {x} with {iter} iterations successfully, gradient's norm is {nrm}.\".format(x=xk,iter=k,nrm=norm(pk)))\n",
    "    else:\n",
    "        print(\"Unable to locate minimizer within maximum iterations, last position is at {x}, gradient's norm is {nrm}\".format(x=xk,nrm=norm(pk)))\n",
    "        \n",
    "    return xk, k, path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wolfe Conditions\n",
    "\n",
    "In general, it is quite expensive to find the optimal $\\alpha_k$ in each iteration. Therefore, we usually use some simple strategies to find a good $\\alpha_k$, which leads to the ``inexact line search``. The most popular one is the so-called ``Wolfe Conditions``.\n",
    "\n",
    "The Wolfe Conditions are two inequalities that the step length $\\alpha_k$ should satisfy:\n",
    "\n",
    "1. **Sufficient decrease condition** (also called **Armijo condition**): $f(x_k + \\alpha_k p_k) \\leq f(x_k) + c_1 \\alpha_k \\nabla f_k^T p_k$ with $0 < c_1 < 1$.\n",
    "2. **Curvature condition**: $\\nabla f(x_k + \\alpha_k p_k)^T p_k \\geq c_2 \\nabla f_k^T p_k$ with $0 < c_1 < c_2 < 1$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence\n",
    "\n",
    "## Newtonâ€™s Method and Quasi-Newton Methods\n",
    "\n",
    "## Stochastic Gradient Descent\n",
    "\n",
    "## Step Length Selection\n",
    "\n",
    "### Exact Line Search\n",
    "\n",
    "### Backtracking Line Search\n",
    "\n",
    "## More Topics\n",
    "\n",
    "### Momentum Methods\n",
    "\n",
    "### Nesterovâ€™s Accelerated Gradient\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Theory\n",
    "\n",
    "### Programming\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
