{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Line Search Methods (Week 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step Length\n",
    "\n",
    "Each iteration of line search methods requires a step length $\\alpha_k$ and a search direction $p_k$ to be computed, the update is\n",
    "\n",
    "$$x_{k+1} = x_k + \\alpha_k p_k.$$\n",
    "\n",
    "Once the search direction $p_k$ is computed, the step length $\\alpha_k$ is then computed to reduce the objective function $f$. Usually, $\\alpha_k$ should be compromised between the reduction of $f$ and the computational cost of computing $\\alpha_k$. Ideally, the best choice is the so-called ``exact line search'' which finds the optimal $\\alpha_k$ that minimizes the following single-variable function $\\phi(\\cdot)$ by\n",
    "\n",
    "$$\\phi(\\alpha) := f(x_k + \\alpha p_k),\\quad \\alpha > 0.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "\"\"\"\n",
    "@description: sample code for exact line search along the direction of pk\n",
    "@parameters : \n",
    "    @objFunc    : objective function  \n",
    "    @xk         : starting point \n",
    "    @pk         : search direction\n",
    "@returns    : step size\n",
    "@note       : https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize_scalar.html\n",
    "\"\"\"\n",
    "def exact_line_search_method(objFunc, xk, pk):\n",
    "    def subproblem1D(alpha):\n",
    "        return objFunc(xk + alpha * pk)\n",
    "    res = minimize_scalar(subproblem1D) \n",
    "    return res.x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wolfe Conditions\n",
    "\n",
    "In general, it is quite expensive to find the optimal $\\alpha_k$ in each iteration. Therefore, we usually use some simple strategies to find a good $\\alpha_k$, which leads to the ``inexact line search``. The most popular one is the so-called ``Wolfe Conditions``.\n",
    "\n",
    "The Wolfe Conditions are two inequalities that the step length $\\alpha_k$ should satisfy:\n",
    "\n",
    "- **Sufficient decrease condition** (also called **Armijo condition**): $f(x_k + \\alpha_k p_k) \\leq f(x_k) + c_1 \\alpha_k \\nabla f_k^T p_k$ with $0 < c_1 < 1$.\n",
    "- **Curvature condition**: $\\nabla f(x_k + \\alpha_k p_k)^T p_k \\geq c_2 \\nabla f_k^T p_k$ with $0 < c_1 < c_2 < 1$.\n",
    "\n",
    "The step length satisfying the Wolfe Conditions is called a **Wolfe step**, which may not be close to the exact step length. In order to force the steo length to be close to the exact step length, we can use the **strong Wolfe Conditions**:\n",
    "\n",
    "- **Sufficient decrease condition**: $f(x_k + \\alpha_k p_k) \\leq f(x_k) + c_1 \\alpha_k \\nabla f_k^T p_k$ with $0 < c_1 < 1$.\n",
    "- **Curvature condition**: $|\\nabla f(x_k + \\alpha_k p_k)^T p_k| \\leq c_2 |\\nabla f_k^T p_k|$ with $0 < c_1 < c_2 < 1$.\n",
    "\n",
    "The only difference is the derivative $\\phi'(\\alpha_k)$ in the curvature condition is replaced by its absolute value $|\\phi'(\\alpha_k)|$, which cannot be too positive.\n",
    "\n",
    "````{prf:lemma} Existence of Wolfe Step\n",
    ":label: lemma-existence-wolfe-step\n",
    "Suppose that $f$ is a continuously differentiable function. Let $p_k$ be a descent direction of $f$ at $x_k$ and assume $f$ is bounded below on the line $x_k + \\alpha p_k$ for $\\alpha > 0$. Then the step length $\\alpha_k$ satisfying the (strong) Wolfe Conditions exists.\n",
    "````\n",
    "\n",
    "````{prf:proof}\n",
    "The proof is omitted here.\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import line_search\n",
    "\n",
    "\"\"\"\n",
    "@description: sample code for wolfe step line search along the direction of pk\n",
    "@parameters : \n",
    "    @objFunc    : objective function  \n",
    "    @objFunc_Grad: gradient of objective function\n",
    "    @xk         : starting point \n",
    "    @pk         : search direction\n",
    "    @c1         : parameter for Armijo condition\n",
    "    @c2         : parameter for curvature condition\n",
    "@returns    : step size\n",
    "@note       : https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.line_search.html\n",
    "\"\"\"\n",
    "\n",
    "def wolfe_line_search_method(objFunc, objFunc_Grad, xk, pk, c1=1e-4, c2=0.9):\n",
    "    return line_search(objFunc, objFunc_Grad, xk, pk, c1=c1, c2=c2)[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence\n",
    "\n",
    "## Newton’s Method and Quasi-Newton Methods\n",
    "\n",
    "## Stochastic Gradient Descent\n",
    "\n",
    "## Step Length Selection\n",
    "\n",
    "### Exact Line Search\n",
    "\n",
    "### Backtracking Line Search\n",
    "\n",
    "## More Topics\n",
    "\n",
    "### Momentum Methods\n",
    "\n",
    "### Nesterov’s Accelerated Gradient\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Theory\n",
    "\n",
    "### Programming\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
