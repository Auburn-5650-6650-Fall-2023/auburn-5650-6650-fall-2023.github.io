{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Line Search Methods (Week 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step Length\n",
    "\n",
    "Each iteration of line search methods requires a step length $\\alpha_k$ and a search direction $p_k$ to be computed, the update is\n",
    "\n",
    "$$x_{k+1} = x_k + \\alpha_k p_k.$$\n",
    "\n",
    "Once the search direction $p_k$ is computed, the step length $\\alpha_k$ is then computed to reduce the objective function $f$. Usually, $\\alpha_k$ should be compromised between the reduction of $f$ and the computational cost of computing $\\alpha_k$. Ideally, the best choice is the so-called ``exact line search'' which finds the optimal $\\alpha_k$ that minimizes the following single-variable function $\\phi(\\cdot)$ by\n",
    "\n",
    "$$\\phi(\\alpha) := f(x_k + \\alpha p_k),\\quad \\alpha > 0.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "\"\"\"\n",
    "@description: sample code for exact line search along the direction of pk\n",
    "@parameters : \n",
    "    @objFunc    : objective function  \n",
    "    @xk         : starting point \n",
    "    @pk         : search direction\n",
    "@returns    : step size\n",
    "@note       : https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize_scalar.html\n",
    "\"\"\"\n",
    "def exact_line_search_method(objFunc, xk, pk):\n",
    "    def subproblem1D(alpha):\n",
    "        return objFunc(xk + alpha * pk)\n",
    "    res = minimize_scalar(subproblem1D) \n",
    "    return res.x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wolfe Conditions\n",
    "\n",
    "In general, it is quite expensive to find the optimal $\\alpha_k$ in each iteration. Therefore, we usually use some simple strategies to find a good $\\alpha_k$, which leads to the ``inexact line search``. The most popular one is the so-called ``Wolfe Conditions``.\n",
    "\n",
    "The Wolfe Conditions are two inequalities that the step length $\\alpha_k$ should satisfy:\n",
    "\n",
    "- **Sufficient decrease condition** (also called **Armijo condition**): $f(x_k + \\alpha_k p_k) \\leq f(x_k) + c_1 \\alpha_k \\nabla f_k^T p_k$ with $0 < c_1 < 1$.\n",
    "- **Curvature condition**: $\\nabla f(x_k + \\alpha_k p_k)^T p_k \\geq c_2 \\nabla f_k^T p_k$ with $0 < c_1 < c_2 < 1$.\n",
    "\n",
    "The step length satisfying the Wolfe Conditions is called a **Wolfe step**, which may not be close to the exact step length. In order to force the steo length to be close to the exact step length, we can use the **strong Wolfe Conditions**:\n",
    "\n",
    "- **Sufficient decrease condition**: $f(x_k + \\alpha_k p_k) \\leq f(x_k) + c_1 \\alpha_k \\nabla f_k^T p_k$ with $0 < c_1 < 1$.\n",
    "- **Curvature condition**: $|\\nabla f(x_k + \\alpha_k p_k)^T p_k| \\leq c_2 |\\nabla f_k^T p_k|$ with $0 < c_1 < c_2 < 1$.\n",
    "\n",
    "The only difference is the derivative $\\phi'(\\alpha_k)$ in the curvature condition is replaced by its absolute value $|\\phi'(\\alpha_k)|$, which cannot be too positive.\n",
    "\n",
    "````{prf:lemma} Existence of Wolfe Step\n",
    ":label: lemma-existence-wolfe-step\n",
    "Suppose that $f$ is a continuously differentiable function. Let $p_k$ be a descent direction of $f$ at $x_k$ and assume $f$ is bounded below on the line $x_k + \\alpha p_k$ for $\\alpha > 0$. Then the step length $\\alpha_k$ satisfying the (strong) Wolfe Conditions exists.\n",
    "````\n",
    "\n",
    "````{prf:proof}\n",
    "The proof is omitted here.\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import line_search\n",
    "\n",
    "\"\"\"\n",
    "@description: sample code for wolfe step line search along the direction of pk\n",
    "@parameters : \n",
    "    @objFunc    : objective function  \n",
    "    @objFunc_Grad: gradient of objective function\n",
    "    @xk         : starting point \n",
    "    @pk         : search direction\n",
    "    @c1         : parameter for Armijo condition\n",
    "    @c2         : parameter for curvature condition\n",
    "@returns    : step size\n",
    "@note       : https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.line_search.html\n",
    "\"\"\"\n",
    "\n",
    "def wolfe_line_search_method(objFunc, objFunc_Grad, xk, pk, c1=1e-4, c2=0.9):\n",
    "    return line_search(objFunc, objFunc_Grad, xk, pk, c1=c1, c2=c2)[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goldstein Conditions\n",
    "\n",
    "Another popular choice of the inexact line search is the **Goldstein Conditions**:\n",
    "\n",
    "$$f(x_k) + c \\alpha_k \\nabla f_k^T p_k \\le f(x_k + \\alpha_k p_k) \\leq f(x_k) + (1 - c) \\alpha_k \\nabla f_k^T p_k$$\n",
    "\n",
    "with $0 < c < \\frac{1}{2}$. The second inequality is the **sufficient decrease condition** and the first inequality is the control of step length from being too short. In comparison with Wolfe condition, one disadvantage of Goldstein condition is that the first inequality of the condition might exclude all minimizers of $\\phi(\\alpha)$. However, usually it is not a fatal problem as long as the objective decreases in the direction of convergence. As a short conclusion, the Goldstein and Wolfe conditions have quite similar convergence theories. Compared to the Wolfe conditions, the Goldstein conditions are often used in Newton-type methods but are not well-suited for quasi-Newton methods that maintain a positive definite Hessian approximation. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backtracking Line Search\n",
    "\n",
    "The **backtracking line search** is a simple strategy to find a step length $\\alpha_k$ that satisfies the sufficient decrease condition. The algorithm is described as follows:\n",
    "\n",
    "````{prf:algorithm} Backtracking Line Search\n",
    ":label: alg-backtracking-line-search\n",
    "\n",
    "Given $x_k$, $p_k$, $\\alpha_0$, $\\rho \\in (0, 1)$, $c \\in (0, 1)$.\n",
    "\n",
    "1. Set $\\alpha = \\alpha_0$.\n",
    "2. While $f(x_k + \\alpha p_k) > f(x_k) + c \\alpha \\nabla f_k^T p_k$ do\n",
    "    1. $\\alpha = \\rho \\alpha$.\n",
    "3. End While\n",
    "````\n",
    "\n",
    "This strategy for terminating a line search is well suited for Newton methods but is less appropriate for quasi-Newton and conjugate gradient methods."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence\n",
    "\n",
    "## Newton’s Method and Quasi-Newton Methods\n",
    "\n",
    "## Stochastic Gradient Descent\n",
    "\n",
    "## More Topics\n",
    "\n",
    "### Momentum Methods\n",
    "\n",
    "### Nesterov’s Accelerated Gradient\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Theory\n",
    "\n",
    "### Programming\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
